import pandas as pd
import nltk
import re
import pathlib
import csv
import openpyxl
from pathlib import Path
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from scipy.sparse import hstack
from numpy import array



# Download VADER sentiment lexicon if not already present
nltk.download('vader_lexicon')

# Initialize the sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

def clean_text(text):
    """
    Clean the input text by removing special characters and lowercasing.
    """
    text = str(text)
    text = re.sub(r'[^\w\s]', '', text).lower()
    return text

def extract_sentiment(text):
    
    scores = analyzer.polarity_scores(text)
    return [scores['neg'], 
            scores['neu'], 
            scores['pos'], 
            scores['compound']]
    




def analyze_dataset(filepath, text_column='text', label_column='label'):
    """
    Analyze a CSV dataset for phishing sentiment and optionally evaluate against true labels.
    """
    path = Path(r"D:\TROY\Web Projects\Thesis\THESIS\data\testDataset.csv")
    if not path.is_file():
        raise FileNotFoundError(f"File not found: {path.resolve()}")
    # Load dataset
    df = pd.read_csv(path)
    
    df['clean_text'] = df[text_column].apply(clean_text)
    
    sentiment_features = df['clean_text'].apply(extract_sentiment).tolist()
    sentiment_df = pd.DataFrame(sentiment_features, columns=['neg','neu','pos','compound'])
     
    tfidf = TfidfVectorizer(max_features=1000)
    X_tfidf = tfidf.fit_transform(df['clean_text'])
    
    X = hstack([X_tfidf, array(sentiment_features)])
    y = df[label_column]
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    print("ML Classification Report:\n", classification_report(y_test, y_pred))
    
    # Get probabilities for “phishing” class 
    phishing_proba = None
    if hasattr(clf, "predict_proba"):
        try:
            idx_phish = list(clf.classes_).index(1)  # 1 = phishing, 0 = not phishing
            phishing_proba = clf.predict_proba(X)[:, idx_phish]
        except ValueError:
            phishing_proba = None


    # Predicted labels for all rows (train+test)
    predicted_labels = clf.predict(X)

    # Assemble the DataFrame:
    result_df = pd.DataFrame({
        'original_text': df[text_column],
        'actual_label': df[label_column],
        'clean_text': df['clean_text'],
        'neg': sentiment_df['neg'],
        'neu': sentiment_df['neu'],
        'pos': sentiment_df['pos'],
        'compound': sentiment_df['compound'],
        'predicted_label': predicted_labels
    })

    # If the probabilities exist, add them:
    if phishing_proba is not None:
        result_df['phishing_probability'] = phishing_proba

    # The metric: predicted_label == 1 means phishing, 0 means not phishing
    result_df['phishing_result'] = result_df['predicted_label'].apply(lambda x: "Phishing" if x == 1 else "Not Phishing")

    return result_df

    

my_csv = Path(r"D:\TROY\Web Projects\Thesis\THESIS\data\testDataset.csv")

if __name__ == '__main__':
    # Change depending on dataset
    csv_path = "testDataset.csv"    # title/where your CSV is
    text_col = "text"               # name of the column with email text
    label_col = "label"             # name of the column with 1/0 or Legit/Phishing

    # runs everything
    final_df = analyze_dataset(csv_path, text_column=text_col, label_column=label_col)

    # Show all columns (or head())
    pd.set_option('display.max_columns', None)
    print(final_df.head(4095))   # out X rows in console
    # If you want to save it (doesnt work idk)
    final_df.to_csv("phishing_analysis_full_results.csv", index=False)

#analyzed_df = analyze_dataset(my_csv)
#pd.set_option('display.max_rows', None)
#print(analyzed_df[['text', 'neg', 'neu', 'pos', 'compound']])

#model = analyze_dataset('data/testDataset.csv', text_column='text', label_column='label')

#result_df, model = analyze_dataset('testDataset.csv', text_column='email_text', label_column='label')
#print(result_df, model)

# Run sentiment phishing detection
# results = df[text_column].apply(detect_phishing_sentiment)
# results_df = pd.DataFrame(results.tolist())

# Combine original data with results
# df = pd.concat([df, results_df], axis=1)

""" def detect_phishing_sentiment(text):
    
     #Assign a phishing probability based on sentiment rules.
    
    text_clean = clean_text(text)
    scores = analyzer.polarity_scores(text_clean)

    # Heuristic phishing probability assignment
    if scores['neg'] > 0.6:
        phishing_probability = 0.8
    elif scores['pos'] > 0.7 and 'urgent' in text_clean:
        phishing_probability = 0.7
    elif scores['compound'] < -0.5:
        phishing_probability = 0.7
    else:
        phishing_probability = 0.2

    return {
        'neg': scores['neg'],
        'neu': scores['neu'],
        'pos': scores['pos'],
        'compound': scores['compound'],
        'phishing_probability': phishing_probability
    }
"""
""" Add predicted label based on phishing threshold
    df['predicted_label'] = df['phishing_probability'].apply(lambda x: 1 if x >= 0.5 else 0)
"""